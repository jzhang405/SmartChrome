# 项目描述：ChromLLM - 基于大模型的智能Chrome插件与后端服务套件

ChromLLM 是一套整合了前端Chrome插件与后端服务的智能交互系统，旨在通过大模型能力为用户提供基于网页内容的实时智能问答与个性化服务。项目采用现代化技术栈构建，兼顾安全性、实时性与可扩展性，具体架构与功能如下：


## 核心定位
依托大语言模型（LLM）能力，打造一个轻量化、高集成的浏览器增强工具：用户可在浏览任意网页时，通过插件快速提取页面内容并发起智能问答（如解析内容、生成摘要、解答疑问等），后端服务则负责处理请求、调用大模型、管理会话，形成"前端交互-内容提取-后端处理-实时反馈"的完整闭环。


## 技术架构与核心功能

### 1. Chrome插件（Manifest V3）
基于最新的Chrome扩展Manifest V3标准开发，兼顾安全性与性能优化：
- **Popup界面**：作为用户交互入口，提供简洁的问答输入框、历史记录展示及基础设置（如模型参数调整），支持用户快速发起提问。
- **Content Script**：注入当前浏览页面的脚本，负责精准提取网页文本内容（支持过滤广告、冗余元素），为大模型问答提供上下文依据，且仅在用户触发时执行，保障隐私安全。
- **Background Service**：作为插件的"后台中枢"，管理WebSocket长连接以维持实时通信状态，处理插件各模块间的消息转发（如Popup与Content Script的数据传递），并负责与后端服务的连接状态监控。
- **通信层**：采用"REST API + WebSocket"混合通信模式——REST API用于非实时请求（如用户登录、历史记录查询），WebSocket用于实时问答交互（确保大模型流式响应能即时推送到前端）。


### 2. Go后端服务
以高性能Go语言为核心，构建稳定、可扩展的后端支撑系统：
- **API网关（Gin框架）**：基于Gin高性能HTTP框架实现接口路由管理，处理前端各类请求（如内容解析、问答提交、用户认证），并集成请求校验、限流等中间件，保障服务稳定性。
- **大模型集成**：封装OpenAI API（或兼容其他LLM接口），支持根据前端传入的网页内容与用户问题生成智能响应，并通过流式输出接口配合WebSocket实现实时反馈。
- **缓存层（Redis）**：缓存高频访问数据（如用户会话信息、近期问答记录、热门网页解析结果），减少重复计算与API调用，提升响应速度。
- **用户会话管理（JWT）**：采用JSON Web Token实现无状态用户认证，支持多设备登录、会话过期控制，保障用户数据安全与访问权限管理。
- **容器化部署（Docker）**：通过Docker封装后端服务及依赖（如Redis），支持一键部署、环境隔离与跨平台运行，简化开发与生产环境的一致性维护。


## 技术亮点
- **Manifest V3适配**：遵循最新浏览器扩展标准，采用Service Worker替代传统背景页，优化资源占用与安全性。
- **实时交互体验**：通过WebSocket+流式响应，实现大模型回答"边生成边展示"，减少用户等待感。
- **高性能后端**：Go语言+Gin框架保障高并发处理能力，Redis缓存降低响应延迟。
- **可扩展性设计**：后端模块解耦，支持替换大模型接口（如接入国产LLM）、扩展缓存策略或增加用户管理功能。


## 应用场景
- 网页内容快速解析（如论文摘要生成、新闻要点提炼）
- 基于当前页面的实时问答（如技术文档解释、外语网页翻译）
- 个性化知识整理（自动保存问答记录，支持后续检索）

ChromLLM 旨在通过技术整合，让大模型能力无缝融入日常浏览场景，提升信息获取与处理效率。